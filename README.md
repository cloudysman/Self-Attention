# Self-Attention-Explanation

My repository contains a detailed explanation and implementation of the Self-Attention mechanism in Transformer models. It includes a Jupyter Notebook for hands-on code demonstration using PyTorch and a PDF document that provides in-depth explanations of the code.

## Files in this Repository

- **Self_attention.ipynb**: A Jupyter Notebook demonstrating the step-by-step implementation of Self-Attention using PyTorch.
  - **Installation of necessary libraries.**
  - **Initialization of Query (Q), Key (K), and Value (V) vectors.**
  - **Calculation of Attention scores using Scaled Dot-Product Attention.**
  - **Application of the Softmax function to obtain Attention weights.**
  - **Computation of the final Attention output.**

- **Giải thích chi tiết về source code.pdf**: A PDF document providing detailed explanations for each part of the source code.
  - **The purpose and functionality of each code segment.**
  - **Step-by-step breakdown of tensor operations.**
  - **Insights into the mathematical concepts behind Self-Attention.**
